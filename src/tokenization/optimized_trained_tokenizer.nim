## Auto-generated optimized tokenizer: OptimizedTrainedTokenizer
## Generated from training corpus using 6x BPE optimizations
## DO NOT EDIT - regenerate using generate_optimized_tokenizer.nim

import std/tables
import ./core

proc newOptimizedTrainedTokenizer*(): Tokenizer =
  ## Create pre-trained optimized tokenizer with compiled merge rules
  result = Tokenizer(kind: tkRegex)
  result.pattern = "'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+"
  result.specialTokens = initTable[string, int]()
  
  # Pre-trained merge rules (compiled at build time)
  result.merges = {
    (32, 115): 312,
    (35, 35): 355,
    (111, 110): 261,
    (259, 258): 281,
    (32, 102): 288,
    (111, 116): 350,
    (262, 103): 311,
    (105, 115): 308,
    (105, 110): 262,
    (102, 105): 329,
    (97, 116): 265,
    (32, 112): 300,
    (101, 120): 318,
    (259, 259): 323,
    (101, 100): 336,
    (101, 114): 266,
    (105, 100): 328,
    (114, 101): 263,
    (256, 32): 258,
    (100, 101): 273,
    (58, 10): 292,
    (32, 34): 297,
    (32, 111): 321,
    (105, 261): 277,
    (32, 39): 293,
    (32, 32): 256,
    (117, 116): 298,
    (299, 110): 344,
    (286, 102): 354,
    (112, 101): 310,
    (97, 110): 278,
    (104, 101): 290,
    (97, 115): 294,
    (259, 264): 347,
    (117, 110): 307,
    (117, 101): 315,
    (273, 102): 335,
    (44, 10): 282,
    (45, 45): 296,
    (41, 10): 272,
    (116, 344): 346,
    (32, 263): 301,
    (270, 116): 306,
    (116, 104): 334,
    (101, 108): 339,
    (260, 114): 337,
    (99, 107): 340,
    (32, 116): 271,
    (32, 291): 317,
    (109, 101): 280,
    (257, 258): 264,
    (256, 256): 257,
    (32, 98): 332,
    (97, 114): 275,
    (32, 119): 352,
    (32, 61): 269,
    (32, 123): 309,
    (289, 102): 291,
    (279, 277): 349,
    (108, 105): 322,
    (99, 116): 279,
    (99, 101): 302,
    (115, 101): 260,
    (41, 292): 343,
    (271, 290): 331,
    (309, 10): 353,
    (32, 105): 286,
    (105, 116): 283,
    (32, 125): 345,
    (32, 97): 327,
    (117, 114): 299,
    (32, 40): 324,
    (108, 111): 304,
    (97, 280): 313,
    (114, 97): 341,
    (260, 108): 289,
    (103, 101): 295,
    (257, 257): 259,
    (115, 116): 268,
    (32, 109): 333,
    (99, 104): 342,
    (32, 110): 305,
    (267, 116): 330,
    (111, 114): 267,
    (117, 108): 338,
    (97, 108): 276,
    (114, 105): 348,
    (32, 262): 320,
    (101, 110): 270,
    (108, 101): 274,
    (59, 10): 316,
    (97, 100): 319,
    (9, 9): 351,
    (10, 10): 287,
    (114, 111): 285,
    (265, 101): 314,
    (32, 100): 325,
    (109, 112): 303,
    (99, 111): 326,
    (32, 99): 284
  }.toTable

  # Pre-built vocabulary (compiled at build time)
  result.vocab = {
    292: ":\n",
    308: "is",
    171: "\xAB",
    260: "se",
    296: "--",
    283: "it",
    178: "\xB2",
    99: "c",
    174: "\xAE",
    125: "}",
    341: "ra",
    205: "\xCD",
    212: "\xD4",
    156: "\x9C",
    12: "\x0C",
    340: "ck",
    145: "\x91",
    98: "b",
    219: "\xDB",
    181: "\xB5",
    353: " {\n",
    101: "e",
    329: "fi",
    35: "#",
    141: "\x8D",
    215: "\xD7",
    333: " m",
    251: "\xFB",
    118: "v",
    77: "M",
    31: "\x1F",
    224: "\xE0",
    34: "\"",
    44: ",",
    153: "\x99",
    355: "##",
    220: "\xDC",
    0: "\0",
    72: "H",
    106: "j",
    73: "I",
    131: "\x83",
    191: "\xBF",
    227: "\xE3",
    184: "\xB8",
    185: "\xB9",
    242: "\xF2",
    65: "A",
    11: "\x0B",
    250: "\xFA",
    70: "F",
    269: " =",
    315: "ue",
    63: "?",
    243: "\xF3",
    92: "\\",
    89: "Y",
    175: "\xAF",
    138: "\x8A",
    210: "\xD2",
    282: ",\n",
    112: "p",
    277: "ion",
    139: "\x8B",
    30: "\x1E",
    279: "ct",
    20: "\x14",
    129: "\x81",
    207: "\xCF",
    236: "\xEC",
    334: "th",
    327: " a",
    262: "in",
    309: " {",
    335: "def",
    194: "\xC2",
    234: "\xEA",
    302: "ce",
    10: "\n",
    66: "B",
    290: "he",
    85: "U",
    121: "y",
    134: "\x86",
    42: "*",
    209: "\xD1",
    94: "^",
    314: "ate",
    45: "-",
    284: " c",
    228: "\xE4",
    276: "al",
    126: "~",
    278: "an",
    130: "\x82",
    135: "\x87",
    172: "\xAC",
    123: "{",
    274: "le",
    6: "\x06",
    248: "\xF8",
    56: "8",
    152: "\x98",
    133: "\x85",
    300: " p",
    110: "n",
    206: "\xCE",
    140: "\x8C",
    311: "ing",
    180: "\xB4",
    183: "\xB7",
    239: "\xEF",
    237: "\xED",
    55: "7",
    195: "\xC3",
    150: "\x96",
    159: "\x9F",
    321: " o",
    27: "\x1B",
    104: "h",
    189: "\xBD",
    216: "\xD8",
    328: "id",
    344: "urn",
    90: "Z",
    21: "\x15",
    57: "9",
    144: "\x90",
    165: "\xA5",
    53: "5",
    79: "O",
    158: "\x9E",
    187: "\xBB",
    22: "\x16",
    38: "&",
    211: "\xD3",
    114: "r",
    29: "\x1D",
    32: " ",
    201: "\xC9",
    137: "\x89",
    214: "\xD6",
    146: "\x92",
    48: "0",
    154: "\x9A",
    41: ")",
    1: "\x01",
    305: " n",
    229: "\xE5",
    169: "\xA9",
    261: "on",
    304: "lo",
    157: "\x9D",
    192: "\xC0",
    74: "J",
    318: "ex",
    258: "   ",
    323: "                ",
    213: "\xD5",
    226: "\xE2",
    347: "               ",
    218: "\xDA",
    109: "m",
    132: "\x84",
    259: "        ",
    265: "at",
    95: "_",
    151: "\x97",
    49: "1",
    287: "\n\n",
    275: "ar",
    167: "\xA7",
    52: "4",
    246: "\xF6",
    306: "ent",
    68: "D",
    51: "3",
    257: "    ",
    61: "=",
    116: "t",
    176: "\xB0",
    232: "\xE8",
    182: "\xB6",
    108: "l",
    291: "self",
    320: " in",
    160: "\xA0",
    50: "2",
    173: "\xAD",
    317: " self",
    217: "\xD9",
    297: " \"",
    7: "\x07",
    299: "ur",
    255: "\xFF",
    238: "\xEE",
    64: "@",
    105: "i",
    120: "x",
    136: "\x88",
    193: "\xC1",
    17: "\x11",
    198: "\xC6",
    343: "):\n",
    352: " w",
    18: "\x12",
    331: " the",
    97: "a",
    177: "\xB1",
    263: "re",
    326: "co",
    23: "\x17",
    143: "\x8F",
    147: "\x93",
    122: "z",
    241: "\xF1",
    124: "|",
    4: "\x04",
    67: "C",
    5: "\x05",
    179: "\xB3",
    348: "ri",
    163: "\xA3",
    39: "'",
    203: "\xCB",
    325: " d",
    103: "g",
    164: "\xA4",
    168: "\xA8",
    46: ".",
    233: "\xE9",
    16: "\x10",
    76: "L",
    78: "N",
    96: "`",
    162: "\xA2",
    190: "\xBE",
    310: "pe",
    324: " (",
    119: "w",
    223: "\xDF",
    339: "el",
    28: "\x1C",
    270: "en",
    249: "\xF9",
    354: " if",
    208: "\xD0",
    60: "<",
    142: "\x8E",
    148: "\x94",
    128: "\x80",
    9: "\t",
    69: "E",
    289: "sel",
    59: ";",
    235: "\xEB",
    286: " i",
    295: "ge",
    71: "G",
    338: "ul",
    37: "%",
    280: "me",
    33: "!",
    75: "K",
    186: "\xBA",
    204: "\xCC",
    319: "ad",
    307: "un",
    264: "       ",
    15: "\x0F",
    316: ";\n",
    231: "\xE7",
    40: "(",
    253: "\xFD",
    245: "\xF5",
    87: "W",
    84: "T",
    312: " s",
    54: "6",
    288: " f",
    342: "ch",
    82: "R",
    271: " t",
    26: "\x1A",
    19: "\x13",
    47: "/",
    166: "\xA6",
    345: " }",
    247: "\xF7",
    80: "P",
    330: "ort",
    91: "[",
    149: "\x95",
    161: "\xA1",
    294: "as",
    107: "k",
    36: "$",
    244: "\xF4",
    200: "\xC8",
    81: "Q",
    298: "ut",
    272: ")\n",
    285: "ro",
    266: "er",
    351: "\t\t",
    281: "           ",
    86: "V",
    93: "]",
    197: "\xC5",
    254: "\xFE",
    336: "ed",
    313: "ame",
    8: "\x08",
    337: "ser",
    346: "turn",
    268: "st",
    230: "\xE6",
    350: "ot",
    100: "d",
    102: "f",
    332: " b",
    111: "o",
    3: "\x03",
    2: "\x02",
    115: "s",
    117: "u",
    127: "\x7F",
    301: " re",
    83: "S",
    13: "\r",
    273: "de",
    225: "\xE1",
    113: "q",
    43: "+",
    14: "\x0E",
    170: "\xAA",
    88: "X",
    240: "\xF0",
    199: "\xC7",
    188: "\xBC",
    155: "\x9B",
    303: "mp",
    267: "or",
    293: " '",
    25: "\x19",
    62: ">",
    252: "\xFC",
    349: "ction",
    222: "\xDE",
    196: "\xC4",
    202: "\xCA",
    256: "  ",
    24: "\x18",
    58: ":",
    221: "\xDD",
    322: "li"
  }.toTable

# Usage example:
# let tokenizer = newOptimizedTrainedTokenizer()
# let tokens = tokenizer.encode("Hello, world!")
# let text = tokenizer.decode(tokens)
