## Auto-generated trained tokenizer: TrainedRegexTokenizer
## Generated from training corpus
## DO NOT EDIT - regenerate using generate_trained_tokenizer.nim

import std/tables
import ./core

proc newTrainedRegexTokenizer*(): Tokenizer =
  ## Create pre-trained regex tokenizer with compiled merge rules
  result = Tokenizer(kind: tkRegex)
  result.pattern = "'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+"
  result.specialTokens = initTable[string, int]()
  
  # Pre-trained merge rules (compiled at build time)
  result.merges = {
    (35, 35): 321,
    (111, 110): 263,
    (259, 258): 292,
    (348, 347): 361,
    (262, 103): 311,
    (105, 110): 262,
    (102, 105): 302,
    (97, 116): 265,
    (101, 120): 342,
    (116, 101): 291,
    (101, 114): 267,
    (32, 269): 284,
    (288, 288): 333,
    (105, 100): 349,
    (105, 282): 335,
    (114, 101): 264,
    (118, 275): 365,
    (100, 105): 312,
    (10, 257): 270,
    (266, 276): 357,
    (39, 261): 364,
    (109, 32): 353,
    (100, 101): 274,
    (328, 97): 370,
    (97, 287): 330,
    (108, 97): 355,
    (279, 46): 290,
    (32, 32): 256,
    (117, 116): 350,
    (272, 116): 369,
    (262, 32): 344,
    (112, 101): 316,
    (109, 97): 334,
    (97, 110): 281,
    (102, 266): 317,
    (97, 115): 298,
    (117, 110): 324,
    (117, 101): 322,
    (351, 263): 372,
    (109, 111): 339,
    (95, 112): 368,
    (45, 45): 288,
    (41, 10): 296,
    (121, 32): 375,
    (101, 32): 271,
    (264, 115): 362,
    (101, 108): 326,
    (116, 104): 280,
    (41, 58): 309,
    (116, 95): 293,
    (260, 114): 332,
    (99, 107): 341,
    (105, 273): 374,
    (109, 101): 287,
    (100, 32): 303,
    (256, 256): 257,
    (97, 114): 277,
    (61, 32): 269,
    (116, 121): 338,
    (115, 40): 373,
    (264, 116): 348,
    (307, 34): 367,
    (108, 105): 308,
    (99, 116): 354,
    (99, 101): 304,
    (263, 101): 371,
    (115, 101): 260,
    (102, 32): 282,
    (259, 257): 285,
    (34, 34): 307,
    (41, 270): 345,
    (115, 32): 273,
    (278, 102): 279,
    (35, 32): 310,
    (115, 105): 314,
    (317, 32): 376,
    (100, 95): 329,
    (117, 114): 297,
    (116, 111): 313,
    (265, 105): 351,
    (108, 111): 301,
    (114, 97): 315,
    (117, 109): 356,
    (116, 267): 359,
    (260, 108): 278,
    (103, 101): 299,
    (116, 105): 283,
    (257, 257): 258,
    (115, 116): 268,
    (99, 104): 306,
    (41, 32): 366,
    (274, 282): 336,
    (111, 114): 266,
    (117, 108): 340,
    (297, 110): 347,
    (280, 271): 331,
    (105, 295): 352,
    (110, 330): 360,
    (97, 108): 275,
    (114, 105): 319,
    (101, 110): 272,
    (115, 95): 346,
    (100, 265): 328,
    (108, 101): 286,
    (116, 40): 325,
    (116, 32): 276,
    (283, 263): 320,
    (97, 100): 343,
    (352, 357): 363,
    (115, 46): 305,
    (10, 10): 323,
    (110, 111): 337,
    (114, 111): 289,
    (99, 263): 327,
    (58, 32): 294,
    (41, 259): 318,
    (265, 101): 358,
    (44, 32): 261,
    (10, 258): 259,
    (109, 112): 295,
    (99, 111): 300
  }.toTable

  # Pre-built vocabulary (compiled at build time)
  result.vocab = {
    292: "\n                ",
    308: "li",
    171: "\xAB",
    260: "se",
    296: ")\n",
    283: "ti",
    178: "\xB2",
    99: "c",
    174: "\xAE",
    125: "}",
    341: "ck",
    205: "\xCD",
    212: "\xD4",
    156: "\x9C",
    12: "\x0C",
    340: "ul",
    145: "\x91",
    98: "b",
    219: "\xDB",
    181: "\xB5",
    353: "m ",
    101: "e",
    329: "d_",
    358: "ate",
    35: "#",
    141: "\x8D",
    215: "\xD7",
    333: "----",
    251: "\xFB",
    118: "v",
    77: "M",
    372: "ation",
    31: "\x1F",
    224: "\xE0",
    34: "\"",
    44: ",",
    153: "\x99",
    355: "la",
    220: "\xDC",
    0: "\0",
    72: "H",
    106: "j",
    73: "I",
    131: "\x83",
    191: "\xBF",
    227: "\xE3",
    184: "\xB8",
    375: "y ",
    185: "\xB9",
    242: "\xF2",
    65: "A",
    11: "\x0B",
    250: "\xFA",
    70: "F",
    269: "= ",
    315: "ra",
    63: "?",
    243: "\xF3",
    92: "\\",
    89: "Y",
    175: "\xAF",
    138: "\x8A",
    210: "\xD2",
    282: "f ",
    112: "p",
    277: "ar",
    139: "\x8B",
    30: "\x1E",
    279: "self",
    20: "\x14",
    129: "\x81",
    361: "return",
    207: "\xCF",
    236: "\xEC",
    334: "ma",
    327: "con",
    262: "in",
    309: "):",
    335: "if ",
    194: "\xC2",
    234: "\xEA",
    302: "fi",
    10: "\n",
    66: "B",
    290: "self.",
    85: "U",
    121: "y",
    134: "\x86",
    42: "*",
    209: "\xD1",
    94: "^",
    314: "si",
    45: "-",
    284: " = ",
    228: "\xE4",
    276: "t ",
    126: "~",
    278: "sel",
    130: "\x82",
    135: "\x87",
    172: "\xAC",
    123: "{",
    274: "de",
    6: "\x06",
    356: "um",
    248: "\xF8",
    56: "8",
    152: "\x98",
    133: "\x85",
    300: "co",
    110: "n",
    206: "\xCE",
    140: "\x8C",
    311: "ing",
    180: "\xB4",
    183: "\xB7",
    362: "res",
    239: "\xEF",
    237: "\xED",
    55: "7",
    195: "\xC3",
    150: "\x96",
    159: "\x9F",
    321: "##",
    27: "\x1B",
    104: "h",
    189: "\xBD",
    216: "\xD8",
    328: "dat",
    344: "in ",
    90: "Z",
    21: "\x15",
    57: "9",
    144: "\x90",
    165: "\xA5",
    53: "5",
    79: "O",
    158: "\x9E",
    374: "is ",
    187: "\xBB",
    22: "\x16",
    38: "&",
    211: "\xD3",
    114: "r",
    29: "\x1D",
    32: " ",
    201: "\xC9",
    137: "\x89",
    214: "\xD6",
    146: "\x92",
    48: "0",
    154: "\x9A",
    41: ")",
    1: "\x01",
    305: "s.",
    229: "\xE5",
    169: "\xA9",
    261: ", ",
    304: "ce",
    157: "\x9D",
    192: "\xC0",
    74: "J",
    318: ")\n        ",
    258: "        ",
    323: "\n\n",
    213: "\xD5",
    226: "\xE2",
    347: "urn",
    218: "\xDA",
    109: "m",
    132: "\x84",
    259: "\n        ",
    265: "at",
    95: "_",
    151: "\x97",
    49: "1",
    287: "me",
    275: "al",
    167: "\xA7",
    52: "4",
    246: "\xF6",
    306: "ch",
    68: "D",
    51: "3",
    257: "    ",
    61: "=",
    116: "t",
    176: "\xB0",
    232: "\xE8",
    182: "\xB6",
    108: "l",
    291: "te",
    320: "tion",
    160: "\xA0",
    50: "2",
    173: "\xAD",
    317: "for",
    217: "\xD9",
    297: "ur",
    376: "for ",
    7: "\x07",
    299: "ge",
    255: "\xFF",
    238: "\xEE",
    64: "@",
    105: "i",
    120: "x",
    136: "\x88",
    193: "\xC1",
    17: "\x11",
    198: "\xC6",
    343: "ad",
    352: "imp",
    18: "\x12",
    331: "the ",
    97: "a",
    177: "\xB1",
    263: "on",
    326: "el",
    23: "\x17",
    143: "\x8F",
    370: "data",
    147: "\x93",
    122: "z",
    241: "\xF1",
    124: "|",
    4: "\x04",
    67: "C",
    5: "\x05",
    179: "\xB3",
    348: "ret",
    163: "\xA3",
    39: "'",
    203: "\xCB",
    325: "t(",
    103: "g",
    164: "\xA4",
    168: "\xA8",
    46: ".",
    233: "\xE9",
    16: "\x10",
    76: "L",
    78: "N",
    96: "`",
    162: "\xA2",
    190: "\xBE",
    310: "# ",
    324: "un",
    119: "w",
    223: "\xDF",
    339: "mo",
    28: "\x1C",
    270: "\n    ",
    365: "val",
    249: "\xF9",
    354: "ct",
    208: "\xD0",
    60: "<",
    142: "\x8E",
    148: "\x94",
    128: "\x80",
    9: "\t",
    69: "E",
    289: "ro",
    59: ";",
    235: "\xEB",
    286: "le",
    295: "mp",
    71: "G",
    338: "ty",
    37: "%",
    280: "th",
    33: "!",
    75: "K",
    186: "\xBA",
    204: "\xCC",
    319: "ri",
    307: "\"\"",
    264: "re",
    15: "\x0F",
    316: "pe",
    231: "\xE7",
    40: "(",
    253: "\xFD",
    245: "\xF5",
    87: "W",
    84: "T",
    371: "one",
    312: "di",
    54: "6",
    288: "--",
    342: "ex",
    82: "R",
    271: "e ",
    26: "\x1A",
    19: "\x13",
    364: "', ",
    366: ") ",
    47: "/",
    166: "\xA6",
    360: "name",
    345: ")\n    ",
    247: "\xF7",
    80: "P",
    330: "ame",
    91: "[",
    149: "\x95",
    161: "\xA1",
    294: ": ",
    107: "k",
    36: "$",
    244: "\xF4",
    359: "ter",
    200: "\xC8",
    81: "Q",
    298: "as",
    272: "en",
    285: "\n            ",
    266: "or",
    351: "ati",
    373: "s(",
    281: "an",
    86: "V",
    93: "]",
    197: "\xC5",
    254: "\xFE",
    336: "def ",
    313: "to",
    8: "\x08",
    337: "no",
    346: "s_",
    268: "st",
    230: "\xE6",
    350: "ut",
    100: "d",
    102: "f",
    332: "ser",
    111: "o",
    367: "\"\"\"",
    3: "\x03",
    369: "ent",
    2: "\x02",
    115: "s",
    117: "u",
    127: "\x7F",
    363: "import ",
    301: "lo",
    83: "S",
    13: "\r",
    273: "s ",
    225: "\xE1",
    113: "q",
    43: "+",
    14: "\x0E",
    170: "\xAA",
    88: "X",
    357: "ort ",
    240: "\xF0",
    199: "\xC7",
    188: "\xBC",
    155: "\x9B",
    303: "d ",
    267: "er",
    293: "t_",
    368: "_p",
    25: "\x19",
    62: ">",
    252: "\xFC",
    349: "id",
    222: "\xDE",
    196: "\xC4",
    202: "\xCA",
    256: "  ",
    24: "\x18",
    58: ":",
    221: "\xDD",
    322: "ue"
  }.toTable

# Usage example:
# let tokenizer = newTrainedRegexTokenizer()
# let tokens = tokenizer.encode("Hello, world!")
# let text = tokenizer.decode(tokens)
